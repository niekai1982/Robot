

- 感知技术支撑学科
  - 计算机科学、人工智能、认知心理学、神经科学，还应包括哲学和数学

- 视觉科学研究领域变得更加技术化、更加数据化，更加“神经化”



#### Q1 : 从内部（CRC或集团）来看，3年或5年后我们是什么样的？能服务什么产业（集团现有业务范围内）？能做多大的贡献？

----

- 仿人感知

- 感知、认知、行为为一体



##### 如何定义感知

- 计算机与机器人感知
  - 传感器
  - 理解 

- 人类感知
  - 更好的感知自己
    - 去探索及研究人类的感知方式
    - 更了解自己
  - 感知的延伸
    - 发觉更多的感知手段，感知维度

##### 服务产业

- 机器人
- 健康产品

##### 贡献

- 

#### Google Research

> Machine Perception



Research in machine perception tackles the hard problems of understanding images, sounds, music and video. In recent years, our computers have become much better at such tasks, enabling a variety of new applications such as: content-based search in Google Photos and Image Search, natural [handwriting](https://googleresearch.blogspot.com/2015/04/google-handwriting-input-in-82.html) interfaces for Android, [optical character recognition](https://googleresearch.blogspot.com/2015/05/paper-to-digital-in-200-languages.html) for Google Drive documents, and recommendation systems that understand music and YouTube videos. Our approach is driven by algorithms that benefit from processing very large, partially-labeled datasets using parallel computing clusters. A good example is our recent work on object recognition using a novel deep convolutional neural network architecture known as [Inception](https://research.google.com/pubs/pub43022.html) that achieves state-of-the-art results on academic benchmarks and allows users to easily search through their large collection of Google Photos. The ability to mine meaningful information from multimedia is broadly applied throughout Google.

https://research.google/teams/perception/



#### Georgia Institute of Technology

Georgia Tech, School of Interactive Computing,Robotics & Computational Perception

> 佐治亚理工学院



Robotics and computational perception research at Georgia Tech runs from engineering to machine learning, from locomotion to autonomous ethical behavior in robotic machines. Our work is focused in two of our research centers and labs: The Robotics and Intelligent Machines (RIM) Center at Georgia Tech and the Computational Perception Lab (CPL).

RIM leverages the Institute’s strengths and resources by reaching across traditional boundaries to embrace a multidisciplinary approach. The College of Computing, College of Engineering and the Georgia Tech Research Institute play key, complementary roles through Tech's traditional expertise in interactive and intelligent computing, control and mechanical engineering. Emphasizing personal and everyday robotics, as well as the future of automation, RIM faculty help students understand and define the future role of robotics in society.

CPL was developed to explore and develop the next generation of intelligent machines, interfaces and environments for modeling, perceiving, recognizing, and interacting with humans. CPL domains of interest include computer vision/perception, computer graphics, computer animation, human-computer interaction, digital special effects, artificial intelligence, pattern recognition, machine learning, robotics, aware home/environments, audio processing, ubiquitous computing/sensing, eldercare technologies, motion analysis, and computational music.



#### Visual Perception Lab of Gerrit Maus

> Gerrit Maus, **于2015年10月在南洋理工大学开始了他的独立科学研究**。

https://www.x-mol.com/news/15762

Extrapolation and Interpolation in Visual Perception

Every day we use our sense of vision to perceive our surroundings and to interact with things in our environment. For successful interactions with things around us, it is essential to have accurate information about where things are. Oftentimes, however, our eyes cannot give us accurate information about where objects around us are located.

![Rafael Nadal](https://tva1.sinaimg.cn/large/0081Kckwgy1gmbmpplytjj308c06k3yr.jpg)

For example, when the objects we want to interact with are moving (e.g. when we try to hit a tennis ball), delays in the pathway from the eyes to the brain mean that our brains receive slightly outdated position information. In other situations, there are large gaps in the information from the eyes, which can be spatial gaps (e.g. when objects fall into the blind spot on the retina) or temporal gaps (e.g. when we blink).

![test](https://blogs.ntu.edu.sg/perception/files/2015/10/Blink-of-an-eye-Super-Slow-Motion-1ldfi3d.mp4?_=1)



Despite these limitations in the sensory input, we excel at interacting with the world around us like no artificial system. We are able to hit fast moving tennis balls despite the delays in our nervous system. We don’t see holes in the world where our blind spots are, and we don’t see the world disappear and reappear with every eye blink. This means that the brain must have evolved sophisticated mechanisms to deal with delays and discontinuities in its sensory input. It is extrapolating and interpolating from the limited available information to build a prediction and a unified conscious percept of the world around us.

![perception-brainscan](https://tva1.sinaimg.cn/large/0081Kckwgy1gmbmpp8thdj308c06ddfy.jpg)

To figure out how these mechanisms work, I study a number of fascinating perceptual illusions and phenomena. In my research, I use behavioral experiments (psychophysics) in healthy participants and patients with brain damage, eye tracking, brain imaging (fMRI), and transcranial magnetic stimulation (TMS) to try to unravel the mysteries of extrapolation and interpolation in visual perception.



#### NVIDIA Robotics Research

https://www.nvidia.com/en-us/research/research-areas/

NVIDIA Research is using artificial intelligence (AI) to enable breakthroughs in robotics that solve real-world problems in industries like manufacturing, logistics, healthcare, and more. The new NVIDIA AI Robotics Research Lab in Seattle is a center of excellence that focuses on areas such as robot manipulation, physics-based simulation, and robot perception.

The goal of the lab is to develop the next generation of robots that can robustly manipulate the physical world and work alongside humans.

##### Associated Publications

[RMPflow: A Geometric Framework for Generation of Multi-Task Motion Policies](https://research.nvidia.com/publication/2020-11_RMPflow-Journal)
[RMPflow: A Computational Graph for Automatic Motion Policy Generation ](https://research.nvidia.com/publication/2020-11_RMPflow)
[Robust Learning of Tactile Force Estimation through Robot Interaction](https://research.nvidia.com/publication/2020-11_Robust-Tactile-Learning)
[Contextual Reinforcement Learning of Visuo-tactile Multi-fingered Grasping Policies](https://research.nvidia.com/publication/2020-11_Contextual-RL)
[Joint Space Control via Deep Reinforcement Learning](https://research.nvidia.com/publication/2020-11_JAiLeR)
[OCEAN: Online Task Inference for Compositional Tasks with Context Adaptation](https://research.nvidia.com/publication/2020-08_OCEAN%3A-Online-Task)
[Indirect Object-to-Robot Pose Estimation from an External Monocular RGB Camera](https://research.nvidia.com/publication/2020-07_Indirect-Object-Pose)
[Automated Synthetic-to-Real Generalization](https://research.nvidia.com/publication/2020-07_Automated-Synthetic-to-Real-Generalization)
[MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views ](https://research.nvidia.com/publication/2020-06_MVLidarNet)
[6-DOF Grasping for Target-driven Object Manipulation in Clutter](https://research.nvidia.com/publication/2020-06_6-DOF-Grasping-for)
[DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm System](https://research.nvidia.com/publication/2020-06_DexPilot)
[Toward Sim-to-Real Directional Semantic Grasping](https://research.nvidia.com/publication/2020-03_directional-semantic-grasping)
[Camera-to-Robot Pose Estimation from a Single Image](https://research.nvidia.com/publication/2020-03_DREAM)
[6-DOF GraspNet: Variational Grasp Generation for Object Manipulation](https://research.nvidia.com/publication/2019-10_6-DOF-GraspNet%3A-Variational)
[Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments](https://research.nvidia.com/publication/2019-06_Putting-Humans-in)
[Fluidic Elastomer Actuators for Haptic Interactions in Virtual Reality](https://research.nvidia.com/publication/2018-12_Fluidic-Elastomer-Actuators)
[Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects](https://research.nvidia.com/publication/2018-09_Deep-Object-Pose)
[Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation](https://research.nvidia.com/publication/2018-09_Learning-Rigidity-in)
[HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration](https://research.nvidia.com/publication/2018-09_HGMM-Registration)
[Simultaneous Edge Alignment and Learning](https://research.nvidia.com/publication/2018-09_Simultaneous-Edge-Alignment)
[Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training](https://research.nvidia.com/publication/2018-09_Domain-Adaptation-for)
[EOE: Expected Overlap Estimation over Unstructured Point Cloud Data](https://research.nvidia.com/publication/2018-09_Probabilistic-Overlap-Estimation)
[Geometry-Aware Learning of Maps for Camera Localization](https://research.nvidia.com/publication/2018-06_Geometry-Aware-Learning-of)
[Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation](https://research.nvidia.com/publication/2018-06_Falling-Things)
[PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes](https://research.nvidia.com/publication/2018-06_PoseCNN%3A-A-Convolutional)
[Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations ](https://research.nvidia.com/publication/2018-05_Learning-Humanreadable-Plans)
[A Variable Shape and Variable Stiffness Controller for Haptic Virtual Interactions](https://research.nvidia.com/publication/2018-04_A-Variable-Shape)
[On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach ](https://research.nvidia.com/publication/2018-04_Semi-Supervised-Stereo)
[Riemannian Motion Policies](https://research.nvidia.com/publication/2018-03_Riemannian-Motion-Policies)
[Sim-to-Real Transfer of Accurate Grasping with Eye-In-Hand Observations and Continuous Control](https://research.nvidia.com/publication/2017-12_Sim-to-Real-Transfer-of)
[Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness](https://research.nvidia.com/publication/2017-09_Toward-Low-Flying-Autonomous)
[Accelerated Generative Models for 3D Point Cloud Data](https://research.nvidia.com/publication/accelerated-generative-models)

